{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG: Retrieval Augmented Generation.\n",
    "- Large language models (LLMs) have a limited context size.\n",
    "- TLDR\n",
    "- Not all context is relevant to a given question\n",
    "- Query -> Search -> Results -> (LLM) -> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sag(search)가 아니라 rag(사람이 retrieval하므로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_text_splitters\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.28 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_text_splitters) (0.1.52)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.28->langchain_text_splitters) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.28->langchain_text_splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.28->langchain_text_splitters) (0.1.59)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.28->langchain_text_splitters) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.28->langchain_text_splitters) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.28->langchain_text_splitters) (8.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hy lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.28->langchain_text_splitters) (2024.2.2)\n",
      "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: langchain_text_splitters\n",
      "Successfully installed langchain_text_splitters-0.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip3 install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader \n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br><p id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</p><br><p id='3' style='font-size:20px'>Sunghun Kim, E. James Whitehead Jr., Member , IEEE , and Yi Zhang, Member , IEEE</p><p id='4' style='font-size:16px'>Abstract —This paper introduces a new technique for predicting latent software bugs, called change classification. Change<br>classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or<br>clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using<br>features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration<br>management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent<br>buggy change recall on average. Change classification has several desirable qualit"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# context를 넣는다는건 탄소를 더 많이 태우는 것. 필요한 부분만 넣자. 그게 RAG.\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. \n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {Context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To answer the question \"What is bug classification?\", we can extract the relevant information from the context.\\n\\nThe context states: \"The change classification technique involves two steps: training and classification. The change classification algorithms learn from a training set, that is, a collection of changes that are known to belong to an existing class, that is, the changes are labeled with the known class. Features are extracted from the changes and the classification algorithm learns which features are the most useful for discriminating among the various classes. In this manner, change classification predicts the existence of bugs in software changes.\"\\n\\nBased on this information, we can conclude that bug classification is a technique for predicting the existence of bugs in software changes by using machine learning classifiers to determine whether a new software change is more similar to prior buggy changes or clean changes. The classifier is trained using features extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a certain accuracy and recall.\\n\\nTherefore, the answer to the question \"What is bug classification?\" is: Bug classification is a technique for predicting the existence of bugs in software changes by using machine learning classifiers to determine whether a new software change is more similar to prior buggy changes or clean changes.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is bug classficiation?\", \"Context\": docs}) # 논문 docs를 context로 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever # 랭체인은 search engine이 있음 쓰면 됨\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language( # 청킹-덩어리화\n",
    "    chunk_size=1000, chunk_overlap=100, language=Language.HTML # 정보를 자르면 개념이 잘릴 수 있으니 overlap을 줌 보통 10% 많으면 50%\n",
    ")\n",
    "splits = text_splitter.split_documents(docs) # doc를 splitter로 청킹\n",
    "\n",
    "retriever = BM25Retriever.from_documents(splits) # 나뉜 splits에서 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"<p id='102' style='font-size:16px'>One assumption of the presentation so far is that a bug is<br>repaired in a single bug-fix change. What happens when a<br>bug is repaired across multiple commits? There are two<br>cases. In the first case, a bug repair is split across multiple<br>commits, with each commit modifying a separate section of<br>the code (code sections are disjoint). Each separate change is<br>tracked back to its initial bug-introducing change, which is<br>then used to train the SVM classifier. In the second case, a bug<br>fix occurs incrementally over multiple commits, with some<br>later fixes modifying earlier ones (the fix code partially<br>overlaps). The first patch in an overlapping code section<br>would be traced back to the original bug-introducing change.<br>Later modifications would not be traced back to the original<br>bug-introducing change. Instead, they would be traced back<br>to an intermediate modification, which is identified as bug\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content='<br>to an intermediate modification, which is identified as bug<br>introducing. This is appropriate since the intermediate<br>modification did not correctly fix the bug and, hence, is<br>simultaneously a bug fix and buggy. In this case, the classifier<br>is being trained with the attributes of the buggy intermediate<br>commit, a valid bug-introducing change.</p><br>', metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content=\"<p id='55' style='font-size:18px'>6. Bug tracking systems for tracking new functional-<br>ities were used. In two of the systems examined,<br>that is, Bugzilla and Scarab, the projects used bug<br>tracking systems to also track new functionality<br>additions to the project. For these projects, the<br>meaning of a bug tracking identifier in the change<br>log message is that either a bug was fixed or a new<br>functionality is added. This substantially increases<br>the number of changes flagged as bug fixes. For<br>these systems, the interpretation of a positive<br>classification output is a change that is either buggy<br>or a new functionality. When using this algorithm,<br>care needs to be taken to understand the meaning of<br>changes identified as bugs and, wherever possible,<br>to ensure that only truly buggy changes are flagged<br>as being buggy.</p><p id='56' style='font-size:20px'>9 C ONCLUSION AND O PEN I SSUES</p><br>\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content='<br>quality, we still are only able to extract a subset of<br>the total number of bugs (typically only 40 percent to<br>60 percent of those reported in the bug tracking<br>system). Since the quality of change logs varies<br>across projects, it is possible that the output of the<br>classification algorithm will include false positives<br>and false negatives. It is currently unclear what<br>impact lower quality change logs has on the<br>classification results.<br>4. The bug-introducing data is incomplete. The SZZ<br>algorithm used to identify bug-introducing changes<br>has limitations: It cannot find bug-introducing<br>changes for bug fixes that only involve the deletion<br>of source code. It also cannot identify bug-introdu-<br>cing changes caused by a change made to a file<br>different from the one being analyzed. It is also<br>possible to miss bug-introducing changes when a file<br>changes its name since the algorithm does not track<br>such name changes.', metadata={'total_pages': 16, 'type': 'html', 'split': 'none'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is bug classficiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The information is not present in the context.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is bug classficiation?\"\n",
    "context_docs = retriever.invoke(query)\n",
    "chain.invoke({\"question\": query, \"Context\": context_docs}) # 쉬운 문제인데 답이 없다네?\n",
    "# 이유는 이 검색기가 keyword 기반이라서, 복잡한 query에 대답을 못줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bug classification is a technique that predicts whether there is a bug in any of the lines that were changed in one file in one SCM commit transaction. It is different from previous bug prediction work that focuses on finding prediction or regression models to identify fault-prone or buggy modules, files, and functions. Bug classification can predict buggy changes as soon as a change is made, which is immediate compared to bug predictions at the module, file, or method level.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is bug classficiation?\"\n",
    "context_docs = retriever.invoke(\"bug\")\n",
    "chain.invoke({\"question\": query, \"Context\": context_docs}) # 답이 나옴 즉 구린 검색기라 keyword로 검색해줘야 함 우리의 needs랑 안 맞음\n",
    "# 그래서 embedding 개념(vector based search)이 필요함\n",
    "# 옛날 검색기의 고질적 문제임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise \n",
    "It seems keyword search is not the best for LLM queries. What are some alternatives?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
